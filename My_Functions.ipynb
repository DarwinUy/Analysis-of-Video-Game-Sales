{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Contents \n",
    "##  : Function to import data using postgress \n",
    "        - SQL_Data(postgres_user = '', postgres_pw = '',\\\n",
    "             postgres_host = '', postgres_port = '',\\\n",
    "             postgres_db ='')\n",
    "##  : Checks for normality in a vector\n",
    "        - NormalityCheck(data, bins = 25, alpha=0.05)\n",
    "##  : Checks For Gauss-Markov Conditions of OLS from with inputs as the Design Matrix and the Target Variable \n",
    "        - GausMarkovCheck(Y, X, alpha= 0.05, bins = 25)\n",
    "## : Plot the correlation heatmap\n",
    "        - cor_map(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install ipynb\n",
    "#! pip install nbloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import data using postgress \n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import pandas as pd\n",
    "    \n",
    "def SQL_Data(postgres_user = '', postgres_pw = '',\\\n",
    "             postgres_host = '', postgres_port = '',\\\n",
    "             postgres_db =''):  \n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    variable = postgres_db\n",
    "    engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
    "    \n",
    "    data = pd.read_sql_query(str('select * from '+ variable),con=engine)\n",
    "    engine.dispose()\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks for normality in a vector\n",
    "\n",
    "# Data: must be a vector we are interested checking\n",
    "# Bins: Number of bins in a \n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import anderson\n",
    "from scipy.stats import jarque_bera\n",
    "\n",
    "def NormalityCheck(data, bins = 25, alpha=0.05):  \n",
    "    fig, ax = plt.subplots(figsize=(18,5), nrows=1, ncols=2)\n",
    "    sns.distplot(data, ax=ax[0], bins=bins)\n",
    "    sm.qqplot(data, stats.norm, ax=ax[1])\n",
    "    plt.show()\n",
    "    print(\"\")\n",
    "    stat, p = shapiro(data)          # test\n",
    "        # interpret\n",
    "    alpha = alpha\n",
    "    if p > alpha:\n",
    "        print('According to Shapiro, the sample looks Gaussian (fail to reject H0)')\n",
    "    else:\n",
    "        print('According to Shapiro, the sample does not look Gaussian (reject H0)')\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    \n",
    "    print(\"\")\n",
    "    stat, p = normaltest(data)          # test\n",
    "    # interpret\n",
    "    alpha = alpha\n",
    "    if p > alpha:\n",
    "        print('According to D’Agostino’s K^2 Normality Test, the sample looks Gaussian (fail to reject H0)')\n",
    "    else:\n",
    "        print('According to D’Agostino’s K^2 Normality Test, the sample does not look Gaussian (reject H0)')\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))             \n",
    "    \n",
    "    print(\"\")\n",
    "    stat, p = jarque_bera(data)          # test\n",
    "    # interpret\n",
    "    alpha = alpha\n",
    "    if p > alpha:\n",
    "        print('According to Jarque Bera, the sample looks Gaussian (fail to reject H0)')\n",
    "    else:\n",
    "        print('According to Jarque Bera, the sample does not look Gaussian (reject H0)')\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks For Gauss-Markov Conditions of OLS from with inputs as the Design Matrix and the Target Variable \n",
    "# Function automatically adds constant\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from scipy.stats import bartlett\n",
    "from scipy.stats import levene\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "## X: Design Matrix; Ensure no missing variables to avoid problems\n",
    "## Y: Target Variable vector\n",
    "## alpha = Significance level of tests\n",
    "## number of bins in histograms\n",
    "\n",
    "def GausMarkovCheck(Y, X, alpha= 0.05, bins = 25):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    results = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "    print(results.summary())\n",
    "\n",
    "    pred_val = results.fittedvalues.copy()\n",
    "    true_val = Y.values.copy()\n",
    "    residual = true_val.ravel() - pred_val.ravel()\n",
    "\n",
    "    print ('')\n",
    "    print ('1) Check Linearity in the coefficients')\n",
    "    if (X.shape[1] % 2) == 0:\n",
    "        nrows = X.shape[1] // 2\n",
    "    else:\n",
    "        nrows = (X.shape[1] // 2) + 1\n",
    "    fig, ax = plt.subplots(figsize=(18,(5*nrows)), ncols=2, nrows=nrows)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for item in list(X.columns):\n",
    "        if nrows != 1:\n",
    "            sns.scatterplot(x=X[item], y=residual, ax=ax[i][j])\n",
    "            ax[i][j].set_ylabel(\"residual\")\n",
    "        else:\n",
    "            sns.scatterplot(x=X[item], y=residual, ax=ax[j])\n",
    "            ax[j].set_ylabel(\"residual\")\n",
    "            \n",
    "        if j < 1 :\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j = 0\n",
    "    plt.show()\n",
    "    \n",
    "    print ('')\n",
    "    print ('2) Check Error term should be 0')\n",
    "    print ('')\n",
    "    stat, p = scipy.stats.ttest_1samp(residual, 0)          # test\n",
    "        # interpret\n",
    "    alpha = alpha\n",
    "    if p > alpha:\n",
    "        print('T-tests suggests the residual mean is 0 (fail to reject H0)')\n",
    "    else:\n",
    "        print('T-tests suggests the residual mean is not 0 (reject H0)')\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    " \n",
    "    print ('')\n",
    "    print ('3) Check for Homoscedacity')\n",
    "    fig, ax = plt.subplots(figsize=(18, 5), nrows=1, ncols=1)\n",
    "    sns.scatterplot(x=pred_val, y=residual)\n",
    "    plt.title('Residual vs Fit')\n",
    "    plt.show()\n",
    "    \n",
    "    print ('')\n",
    "    stat, p = bartlett(pred_val, residual)          # test\n",
    "    # interpret\n",
    "    if p > alpha:\n",
    "        print('According to Bartlett\\'s Test, the sample looks Homoscedastic (fail to reject H0)')\n",
    "    else:\n",
    "        print('According to Bartlett\\'s Test, the sample does not look Homoscedastic (reject H0)')\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    \n",
    "    print(\"\")\n",
    "    stat, p = levene(pred_val, residual)          # test\n",
    "    # interpret\n",
    "    if p > alpha:\n",
    "        print('According to Levene\\'s Test, the sample looks Homoscedastic (fail to reject H0)')\n",
    "    else:\n",
    "        print('According to Levene\\'s Test, the sample does not look Homoscedastic (reject H0)')\n",
    "    print('Statistics=%.3f, p=%.3f' % (stat, p))  \n",
    "    \n",
    "    print ('')\n",
    "    print ('4) Check for Uncorrolated Error terms')\n",
    "    acf_data = acf(residual)\n",
    "    fig, ax = plt.subplots(figsize=(18,5), ncols=2, nrows=1)\n",
    "    ax[0].plot(residual)\n",
    "    ax[0].set_title('Line Plot of Residuals')\n",
    "    ax[1].plot(acf_data[1:])\n",
    "    ax[1].set_title('Autocorrelation Plot of Residuals')\n",
    "    plt.show()\n",
    "    \n",
    "    print ('')\n",
    "    print ('5 & 6) Check for Multicolinearity and Feature correlation with error')\n",
    "    \n",
    "    print ('')\n",
    "    print('Variance Inflation Factors')\n",
    "    print(pd.Series([variance_inflation_factor(add_constant(X.select_dtypes([np.number]).dropna()).values, i) \\\n",
    "                    for i in range(add_constant(X.select_dtypes([np.number]).dropna()).shape[1])],\\\n",
    "                    index=(add_constant(X.select_dtypes([np.number]).dropna()).columns)))\n",
    "    \n",
    "    cor_map(pd.concat([X, pd.DataFrame(residual).rename(columns={0: 'residuals'})], axis=1))\n",
    "    \n",
    "    print ('')\n",
    "    print ('7) Check for Normality of the Error')    \n",
    "    NormalityCheck(residual, bins = bins, alpha=alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation heatmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Data: The data set we are interested in checking  the correlation for\n",
    "\n",
    "def cor_map(data): \n",
    "    fig, ax = plt.subplots(figsize=(13,13), nrows=1, ncols=1)\n",
    "    corrmat = data.corr()\n",
    "    sns.heatmap(corrmat, center=0, square=True, annot=True, linewidths=.5)\n",
    "    plt.title('correlation matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get transformed data from PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def PCA_Trans(data, n):\n",
    "    pca = PCA(n_components=n)   #if we want to to select number of components , can also look at % variance saved if 0< n_components < 1\n",
    "    pca.fit(data.dropna())\n",
    "    pca_expenditure = pca.transform(data.dropna())\n",
    "    return(pd.DataFrame(pca_expenditure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## histogram of all data\n",
    "def All_Hist(data, bins, norm_hist=False):\n",
    "    if (data.select_dtypes([np.number]).shape[1] % 3) == 0:\n",
    "        nrows = data.select_dtypes([np.number]).shape[1] // 3\n",
    "    else:\n",
    "        nrows = (data.select_dtypes([np.number]).shape[1] // 3) + 1\n",
    "    fig, ax = plt.subplots(figsize=(18,(5*nrows)), ncols=3, nrows=nrows)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for item in list(data.select_dtypes([np.number]).columns):\n",
    "        if nrows != 1:\n",
    "            sns.distplot(data.select_dtypes([np.number])[item], ax=ax[i][j], bins=bins, norm_hist=norm_hist)\n",
    "            ax[i][j].set_ylabel(item)\n",
    "        else:\n",
    "            sns.distplot(data.select_dtypes([np.number])[item], ax=ax[j], bins=bins, norm_hist=norm_hist)\n",
    "            ax[j].set_ylabel(\"residual\")\n",
    "        if j < 2 :\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j = 0\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatter plot of Y vs all data columns\n",
    "# Y= Target variable\n",
    "#data=dataset\n",
    "# hue= hue, categorical variable\n",
    "def All_Scatter(Y, data, hue=None):\n",
    "    if (data.drop([Y], axis=1).select_dtypes([np.number]).shape[1] % 3) == 0:\n",
    "        nrows = data.select_dtypes([np.number]).shape[1] // 3\n",
    "    else:\n",
    "        nrows = (data.drop([Y], axis=1).select_dtypes([np.number]).shape[1] // 3) + 1\n",
    "    fig, ax = plt.subplots(figsize=(18,(5*nrows)), ncols=3, nrows=nrows)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for item in list(data.drop([Y], axis=1).select_dtypes([np.number]).columns):\n",
    "        if nrows != 1:\n",
    "            sns.scatterplot(y=Y, x=item, data=data, ax=ax[i][j], hue=hue)\n",
    "            ax[i][j].set_ylabel(Y)\n",
    "        else:\n",
    "            sns.scatterplot(y=Y, x=item, data=data, ax=ax[j], hue=hue)\n",
    "            ax[j].set_ylabel(Y)\n",
    "        if j < 2 :\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j = 0\n",
    "    plt.show()\n",
    "    \n",
    "# if problems, remove hue from each part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplots of all categoricals/objects\n",
    "\n",
    "def All_Boxen(Y, data):\n",
    "    if (data.drop([Y], axis=1).select_dtypes([np.object]).shape[1] % 3) == 0:\n",
    "        nrows = data.select_dtypes([np.object]).shape[1] // 3\n",
    "    else:\n",
    "        nrows = (data.drop([Y], axis=1).select_dtypes([np.object]).shape[1] // 3) + 1\n",
    "    fig, ax = plt.subplots(figsize=(18,(5*nrows)), ncols=3, nrows=nrows)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for item in list(data.drop([Y], axis=1).select_dtypes([np.object]).columns):\n",
    "        if nrows != 1:\n",
    "            sns.boxenplot(y=Y, x=item, data=data, ax=ax[i][j])\n",
    "            ax[i][j].set_ylabel(Y)\n",
    "        else:\n",
    "            sns.boxenplot(y=Y, x=item, data=data, ax=ax[j])\n",
    "            ax[j].set_ylabel(Y)\n",
    "        if j < 2 :\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j = 0\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See number and percentage of variables that have missing values \n",
    "\n",
    "def Missing_Var(data):\n",
    "    return(pd.concat([data.isna().sum().rename('# Missing'), \n",
    "    ((data.isna().sum())/data.shape[0]).rename('% Missing')], axis = 1 )[list(data.isna().sum()>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Variance Inflation Factors\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# automatically drops NA and inserts constant\n",
    "def VIF(X):\n",
    "    print(pd.Series([variance_inflation_factor(add_constant(X.select_dtypes([np.number]).dropna()).values, i) \\\n",
    "                    for i in range(add_constant(X.select_dtypes([np.number]).dropna()).shape[1])],\\\n",
    "                    index=(add_constant(X.select_dtypes([np.number]).dropna()).columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection by VIF\n",
    "# data = design matrix\n",
    "# cutoff\n",
    "\n",
    "def VIF_selection(data, cutoff=5):\n",
    "    Cdata=data   #clean data\n",
    "    X= add_constant(Cdata.select_dtypes([np.number]).dropna())\n",
    "    VarFac=pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=(X.columns))    \n",
    "    while max(VarFac[VarFac.index.values != 'const']) > cutoff:\n",
    "        X= add_constant(Cdata.select_dtypes([np.number]).dropna())\n",
    "        VarFac=pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=(X.columns))\n",
    "        Cdata=Cdata.drop([np.argmax(VarFac[VarFac.index.values != 'const'])], axis = 1)\n",
    "    return(Cdata, VIF(Cdata))\n",
    "\n",
    "# Cdata =  selected data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a list into random groups\n",
    "    # done by list generation, not for loops\n",
    "\n",
    "#a: list\n",
    "#n: number of sets\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "# notes:\n",
    "#k: numerator of len(a)/n\n",
    "#m: remainder of len(a)/n \n",
    "    \n",
    "# notes:\n",
    "# if remainder after division is shorter of greater will decide final list\n",
    "# The real problem is looking ate the final list\n",
    "#\n",
    "# credit\n",
    "# https://stackoverflow.com/questions/3352737/python-randomly-partition-a-list-into-n-nearly-equal-parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in data and cross validates \n",
    "\n",
    "from random import shuffle\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#Target:Target colun\n",
    "#Data:Design matrix with target attatched as colummn\n",
    "#n: nuumber of cross validation\n",
    "\n",
    "def Cross_Val(data, n=5, Target='Positive'):\n",
    "    S_Index = list(range(data.shape[0]))             # get observation indices\n",
    "    shuffle(S_Index)                               #shuffle the data points\n",
    "    k, m = divmod(len(S_Index), n)\n",
    "    g=[S_Index[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]    # g: list of divided lists, division\n",
    "    for item in range(n):\n",
    "        g_test=g[item]\n",
    "        g_train=g[:item]+g[item+1:]\n",
    "        g_train = [item for sublist in g_train for item in sublist]\n",
    "        \n",
    "        bnb = BernoulliNB()         # bernouli Naive Bays\n",
    "        # Fit our model to the data.\n",
    "        bnb.fit((data.iloc[g_train]).drop(columns=[Target]), data.iloc[g_train][Target])\n",
    "        \n",
    "        # Classify, storing the result in a new variable.\n",
    "        y_pred = bnb.predict((data.iloc[g_test]).drop(columns=[Target]))\n",
    "        print((((data.iloc[g_test])[Target] != y_pred).sum())/len(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
